{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class buildModel():\n",
    "    def __init__(self):\n",
    "        self.train_model()\n",
    "    \n",
    "    def load_cfar10_batch(self, cifar10_dataset_folder_path, batch_id):\n",
    "        with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
    "            # note the encoding type is 'latin1'\n",
    "            batch = pickle.load(file, encoding='latin1')\n",
    "\n",
    "        features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "        labels = batch['labels']\n",
    "        return features, labels\n",
    "    \n",
    "    \n",
    "    def get_batches(self):\n",
    "        features, labels = self.load_cfar10_batch('E:\\images-data\\cifar-10-batches-py', 1)\n",
    "        for i in range(2, 6):\n",
    "            f, l = self.load_cfar10_batch('E:\\images-data\\cifar-10-batches-py', i)\n",
    "            features = np.vstack((features, f))\n",
    "            labels = np.hstack((labels, l))\n",
    "        return features, labels\n",
    "    \n",
    "    \n",
    "    def normalize_get_data(self):\n",
    "        \n",
    "        # Get batches\n",
    "        features, labels = self.get_batches()\n",
    "        \n",
    "        # Normalize data\n",
    "        mean = features.mean()\n",
    "        std = features.std()\n",
    "        features = (features - mean)/std\n",
    "        spliter = 5000\n",
    "        train_x, val_x = features[0:spliter], features[spliter:6000]\n",
    "        train_y, val_y = labels[0:spliter], labels[spliter:6000]\n",
    "        \n",
    "        return {\"data\":(train_x, train_y, val_x, val_y), \"statistic\":(mean, std)}\n",
    "    \n",
    "    \n",
    "    \n",
    "    def inception_block(self, x):\n",
    "        a_1 = keras.layers.Conv2D(64, (1,1), strides=1, padding='same', data_format='channels_last', )(x)\n",
    "        a_1 = keras.layers.BatchNormalization(axis=3, momentum=.9)(a_1)\n",
    "        a_1 = keras.layers.Activation(\"relu\")(a_1)\n",
    "\n",
    "        a_2 = keras.layers.Conv2D(96, (1,1), strides=1, padding='same', data_format='channels_last')(x)\n",
    "        a_2 = keras.layers.BatchNormalization(axis=3, momentum=.9)(a_2)\n",
    "        a_2 = keras.layers.Activation(\"relu\")(a_2)\n",
    "        a_2 = keras.layers.Conv2D(128, (3,3), strides=1, padding='same', data_format='channels_last')(a_2)\n",
    "        a_2 = keras.layers.BatchNormalization(axis=3, momentum=.9)(a_2)\n",
    "        a_2 = keras.layers.Activation(\"relu\")(a_2)\n",
    "\n",
    "        a_3 = keras.layers.Conv2D(16, (1,1), strides=1, padding='same', data_format='channels_last')(x)\n",
    "        a_3 = keras.layers.BatchNormalization(axis=3, momentum=.9)(a_3)\n",
    "        a_3 = keras.layers.Activation(\"relu\")(a_3)\n",
    "        a_3 = keras.layers.Conv2D(32, (5,5), strides=1, padding='same', data_format='channels_last')(a_3)\n",
    "        a_3 = keras.layers.BatchNormalization(axis=3, momentum=.9)(a_3)\n",
    "        a_3 = keras.layers.Activation(\"relu\")(a_3)\n",
    "\n",
    "        a_4 = keras.layers.MaxPooling2D((1, 1), padding='same')(x)\n",
    "        a_4 = keras.layers.Conv2D(32, (1,1), strides=1, padding='same', data_format='channels_last')(a_4)\n",
    "        a_4 = keras.layers.Activation(\"relu\")(a_4)\n",
    "        \n",
    "        out = keras.layers.Concatenate(axis=-1)([a_1, a_2, a_3, a_4])\n",
    "        return out\n",
    "\n",
    "    def max_pool(self, x):\n",
    "        x = keras.layers.MaxPooling2D((2, 2))(x)\n",
    "        return x \n",
    "    \n",
    "    \n",
    "    def model(self, input_shape):\n",
    "        input_x = keras.layers.Input(shape=input_shape)\n",
    "        x = keras.layers.Conv2D(32, (3, 3), strides = 1, padding='same', data_format = \"channels_last\", activation=\"relu\")(input_x)\n",
    "        x = self.max_pool(x)\n",
    "        x = keras.layers.BatchNormalization(momentum = .9)(x)\n",
    "        \n",
    "        x = self.inception_block(input_x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = keras.layers.Conv2D(16, (1,1), strides=1, padding='same', data_format='channels_last', activation=\"relu\")(x)\n",
    "\n",
    "        x = keras.layers.AveragePooling2D((3,3), strides=2)(x)\n",
    "        \n",
    "        x= keras.layers.Flatten()(x)\n",
    "\n",
    "        x = keras.layers.Dropout(.5)(x)\n",
    "\n",
    "        x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "\n",
    "        x = keras.layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "        model = keras.models.Model(input_x, x)\n",
    "\n",
    "        return model\n",
    "\n",
    "   \n",
    "   \n",
    "    def plot(self, history, file_name):\n",
    "        # Plot training & validation accuracy values\n",
    "        file_name_acc = file_name + \"_acc.png\"\n",
    "        plt.plot(history.history['acc'])\n",
    "        plt.plot(history.history['val_acc'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Val'], loc='upper left')\n",
    "        plt.savefig(file_name_acc)\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot training & validation loss values\n",
    "        file_name_loss = file_name + \"_loss.png\"\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Val'], loc='upper left')\n",
    "        plt.savefig(file_name_loss)\n",
    "        plt.close()\n",
    "        \n",
    "    def save_info(self, history, lr, beta, batch_size, file_name):\n",
    "    \n",
    "        info = \" lr = {} \\n beta = {} \\n batch_size = {} \\n acc = {} \\n val_acc = {} \\n loss = {} \\n val_loss = {}\".format(\n",
    "        lr, beta, batch_size, history.history['acc'][-1], history.history['val_acc'][-1],\\\n",
    "            history.history['loss'][-1], history.history['val_loss'][-1])\n",
    "\n",
    "        file_name += \".txt\"\n",
    "        file = open(file_name, 'w')\n",
    "        file.write(info)\n",
    "        file.close()\n",
    "    \n",
    "    def train_model(self):\n",
    "        # Get data\n",
    "        raw_data = self.normalize_get_data()\n",
    "        train_x, train_y, val_x, val_y = raw_data[\"data\"]\n",
    "        cifarModel = self.model(train_x[0].shape)\n",
    "        \n",
    "        train_x = train_x[:100]\n",
    "        train_y = train_y[:100]\n",
    "        \n",
    "        \n",
    "        # Loop over optimizers\n",
    "        for batch_size in [64, 128, 256]:\n",
    "            for lr in [.01, .001, .0001]:\n",
    "                for beta in [.9, .99, .999]:\n",
    "                    print(\"batch_size is {}, lr is {}, beta is {}\".format(batch_size, lr, beta))\n",
    "                    optimizer = keras.optimizers.Adam(lr=lr, beta_1=beta, beta_2=beta, epsilon=10e-8, decay=0.0, amsgrad=False)\n",
    "                    cifarModel.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "                    file_name = \"results/lr_\"+ str(lr) + \"_beta_\" + str(beta) + \"_batchsize_\" + str(batch_size)\n",
    "                    checkpointer = keras.callbacks.ModelCheckpoint(filepath=file_name +\".hdf5\", verbose=0, save_best_only=True)\n",
    "                    history = cifarModel.fit(x= train_x, y=keras.utils.to_categorical(train_y), \\\n",
    "                                   batch_size=batch_size, epochs=200, validation_data=(val_x, keras.utils.to_categorical(val_y)),\\\n",
    "                                   callbacks=[checkpointer])\n",
    "                    self.plot(history, file_name)\n",
    "                    self.save_info(history, lr, beta, batch_size, file_name)\n",
    "                    \n",
    "        return cifarModel.summary()            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size is 64, lr is 0.01, beta is 0.9\n",
      "Train on 100 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 3.2871 - acc: 0.1000 - val_loss: 3.5902 - val_acc: 0.0930\n"
     ]
    }
   ],
   "source": [
    "obj = buildModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 2005s 50ms/step - loss: 1.7454 - acc: 0.3688 - val_loss: 1.4217 - val_acc: 0.4902\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 2009s 50ms/step - loss: 1.4451 - acc: 0.4780 - val_loss: 1.2707 - val_acc: 0.5434\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 2108s 53ms/step - loss: 1.3354 - acc: 0.5207 - val_loss: 1.2104 - val_acc: 0.5657\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 2193s 55ms/step - loss: 1.2571 - acc: 0.5507 - val_loss: 1.1074 - val_acc: 0.6050\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 2089s 52ms/step - loss: 1.1989 - acc: 0.5708 - val_loss: 1.0822 - val_acc: 0.6172\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 2061s 52ms/step - loss: 1.1664 - acc: 0.5825 - val_loss: 1.0390 - val_acc: 0.6309\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 2037s 51ms/step - loss: 1.1312 - acc: 0.5944 - val_loss: 1.0297 - val_acc: 0.6425\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 2653s 66ms/step - loss: 1.1032 - acc: 0.6082 - val_loss: 0.9793 - val_acc: 0.6515\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 1980s 50ms/step - loss: 1.0809 - acc: 0.6124 - val_loss: 0.9791 - val_acc: 0.6548\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 1979s 49ms/step - loss: 1.0606 - acc: 0.6202 - val_loss: 0.9297 - val_acc: 0.6710\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 2037s 51ms/step - loss: 1.0431 - acc: 0.6289 - val_loss: 0.9579 - val_acc: 0.6624\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 2180s 54ms/step - loss: 1.0254 - acc: 0.6347 - val_loss: 0.9024 - val_acc: 0.6846\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 2062s 52ms/step - loss: 1.0088 - acc: 0.6406 - val_loss: 0.8888 - val_acc: 0.6860\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 2049s 51ms/step - loss: 0.9961 - acc: 0.6451 - val_loss: 0.8891 - val_acc: 0.6912\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 2111s 53ms/step - loss: 0.9769 - acc: 0.6505 - val_loss: 0.8870 - val_acc: 0.6888\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 2126s 53ms/step - loss: 0.9709 - acc: 0.6548 - val_loss: 0.8682 - val_acc: 0.6961\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 2176s 54ms/step - loss: 0.9554 - acc: 0.6594 - val_loss: 0.8708 - val_acc: 0.6959\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 2129s 53ms/step - loss: 0.9575 - acc: 0.6593 - val_loss: 0.8424 - val_acc: 0.7047\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 2049s 51ms/step - loss: 0.9390 - acc: 0.6637 - val_loss: 0.8497 - val_acc: 0.7018\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 2077s 52ms/step - loss: 0.9336 - acc: 0.6654 - val_loss: 0.8410 - val_acc: 0.7058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x187a7830d30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifarModel.fit(x= train_x, y=keras.utils.to_categorical(train_y), batch_size=64, epochs=20, validation_data=(val_x, keras.utils.to_categorical(val_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
