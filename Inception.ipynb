{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work presents implemeting Inception model using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color:red'>Documentation </h3>\n",
    "<pre>\n",
    "**class Model: Has all methods for geting data, perpare it, build\n",
    "               the structure of NN, train and evaluate it.**\n",
    "*Methods*:\n",
    "               \n",
    "\n",
    "\n",
    "- get_data          : This function for Loading cifar data - \n",
    "                      images and labels.\n",
    "- normalize_get_data: This function for normalizing the data and \n",
    "                      split it.\n",
    "- inception_block   : This function for building the inception block.  \n",
    "- model             : This function for building the whole model.\n",
    "- plot              : This function for plotting figures, descriping \n",
    "                      the realtion between accuracy and epochs.\n",
    "- save_info         : This function to save info. about the model.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.train_model()\n",
    "    \n",
    "    \n",
    "    def get_data(self):\n",
    "        '''\n",
    "        This function for reading the data from the file and store it\n",
    "        in numpy array as images and labels\n",
    "        \n",
    "        returns:-\n",
    "        images: numpy array of size (50000, 32, 32, 3).\n",
    "        lable : numpy array of size (50000, 1). \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # For storeing the output of the function\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        # Loop through each file in the folder to be read.\n",
    "        for batch_id in range(1, 6):\n",
    "            \n",
    "            # Open the file and read the data.\n",
    "            with open('cifar-10-batches-py' + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
    "                # note the encoding type is 'latin1'\n",
    "                batch = pickle.load(file, encoding='latin1')\n",
    "            \n",
    "            # append each patch to other patches\n",
    "            images.append(batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1).astype(\"uint8\"))\n",
    "            labels.append(batch['labels'])\n",
    "        \n",
    "        # Now the images array has the size (5, 10000, 32, 32, 3) let's reshape it \n",
    "        # to the shape (50000, 32, 32, 3). Also reshaping labels array to have the \n",
    "        # size (50000,1)\n",
    "        images = np.reshape(np.array(images), (50000, 32, 32, 3))        \n",
    "        labels = np.reshape(np.array(labels), (50000, -1))\n",
    "        \n",
    "        # return the result.\n",
    "        return images, labels\n",
    "    \n",
    "    \n",
    "    def normalize_get_data(self):\n",
    "        '''\n",
    "        This function for normalize input between [-1, 1] \n",
    "        using mean and std. \n",
    "        parmas: no.\n",
    "        returns: \n",
    "                train_x : numpy array of size (5000, 32, 32, 3).\n",
    "                val_x   : numpy array of size (1000, 32, 32, 3).\n",
    "                train_y : numpy array of size (5000, 1). \n",
    "                val_y   : numpy array of size (1000, 1).\n",
    "        \n",
    "        '''\n",
    "        # Get batches\n",
    "        features, labels = self.get_data()\n",
    "        \n",
    "        # Normalize data\n",
    "        mean = features.mean()\n",
    "        std = features.std()\n",
    "        features = (features - mean)/std\n",
    "        \n",
    "        '''\n",
    "        Split the data into training set and testing set.\n",
    "        In this code training set is 5000 and validation set =1000.\n",
    "        NOTE:\n",
    "        YOU CAN CHANGE THE SIZE OF DATA.\n",
    "        '''\n",
    "        spliter = 5000\n",
    "        train_x, val_x = features[0:spliter], features[spliter:6000]\n",
    "        train_y, val_y = labels[0:spliter], labels[spliter:6000]\n",
    "    \n",
    "        \n",
    "        return {\"data\":(train_x, train_y, val_x, val_y), \"statistic\":(mean, std)}\n",
    "    \n",
    "    \n",
    "    def inception_block(self, x):\n",
    "        '''\n",
    "        This function for building just inception block and \n",
    "        calculate output of it as descriped in readme. \n",
    "        params:\n",
    "            x: features.\n",
    "        returns:\n",
    "            output of inception block. \n",
    "        '''\n",
    "        \n",
    "        a_1 = keras.layers.Conv2D(64, (1,1), strides=1, padding='same', data_format='channels_last', )(x)\n",
    "        a_1 = keras.layers.BatchNormalization(axis=3, momentum=.9)(a_1)\n",
    "        a_1 = keras.layers.Activation(\"relu\")(a_1)\n",
    "\n",
    "        a_2 = keras.layers.Conv2D(96, (1,1), strides=1, padding='same', data_format='channels_last')(x)\n",
    "        a_2 = keras.layers.BatchNormalization(axis=3, momentum=.9)(a_2)\n",
    "        a_2 = keras.layers.Activation(\"relu\")(a_2)\n",
    "        a_2 = keras.layers.Conv2D(128, (3,3), strides=1, padding='same', data_format='channels_last')(a_2)\n",
    "        a_2 = keras.layers.BatchNormalization(axis=3, momentum=.9)(a_2)\n",
    "        a_2 = keras.layers.Activation(\"relu\")(a_2)\n",
    "\n",
    "        a_3 = keras.layers.Conv2D(16, (1,1), strides=1, padding='same', data_format='channels_last')(x)\n",
    "        a_3 = keras.layers.BatchNormalization(axis=3, momentum=.9)(a_3)\n",
    "        a_3 = keras.layers.Activation(\"relu\")(a_3)\n",
    "        a_3 = keras.layers.Conv2D(32, (5,5), strides=1, padding='same', data_format='channels_last')(a_3)\n",
    "        a_3 = keras.layers.BatchNormalization(axis=3, momentum=.9)(a_3)\n",
    "        a_3 = keras.layers.Activation(\"relu\")(a_3)\n",
    "\n",
    "        a_4 = keras.layers.MaxPooling2D((1, 1), padding='same')(x)\n",
    "        a_4 = keras.layers.Conv2D(32, (1,1), strides=1, padding='same', data_format='channels_last')(a_4)\n",
    "        a_4 = keras.layers.Activation(\"relu\")(a_4)\n",
    "        \n",
    "        # Concancate all the layers.\n",
    "        out = keras.layers.Concatenate(axis=-1)([a_1, a_2, a_3, a_4])\n",
    "        return out\n",
    "\n",
    "    def max_pool(self, x):\n",
    "        \n",
    "        '''\n",
    "        This function for computing maxPooling that shrinks the \n",
    "        size by two.\n",
    "        params:\n",
    "            x: input from previous layer.\n",
    "        returns:\n",
    "            the out from maxPlooling.\n",
    "        \n",
    "        '''\n",
    "        x = keras.layers.MaxPooling2D((2, 2))(x)\n",
    "        return x \n",
    "    \n",
    "    \n",
    "    def model(self, input_shape):\n",
    "        '''\n",
    "        This function for building the whole model as persented\n",
    "        in the paper except that I used just one block of \n",
    "        inception model.\n",
    "        \n",
    "        params:\n",
    "            input_shape: Tuple of the input shape. \n",
    "        returns:\n",
    "            model: the instamce of the model\n",
    "        '''\n",
    "        \n",
    "        input_x = keras.layers.Input(shape=input_shape)\n",
    "        x = keras.layers.Conv2D(32, (3, 3), strides = 1, padding='same', data_format = \"channels_last\", activation=\"relu\")(input_x)\n",
    "        x = self.max_pool(x)\n",
    "        x = keras.layers.BatchNormalization(momentum = .9)(x)\n",
    "        \n",
    "        x = self.inception_block(input_x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = keras.layers.Conv2D(16, (1,1), strides=1, padding='same', data_format='channels_last', activation=\"relu\")(x)\n",
    "\n",
    "        x = keras.layers.AveragePooling2D((3,3), strides=2)(x)\n",
    "        \n",
    "        x= keras.layers.Flatten()(x)\n",
    "\n",
    "        x = keras.layers.Dropout(rate = 1 - .5)(x)\n",
    "\n",
    "        x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "\n",
    "        x = keras.layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "        model = keras.models.Model(input_x, x)\n",
    "\n",
    "        return model\n",
    "\n",
    "   \n",
    "   \n",
    "    def plot(self, history, file_name):\n",
    "        \n",
    "        '''\n",
    "        This function for ploting acc vs epochs \n",
    "        and loss vs epochs.\n",
    "        \n",
    "        input: \n",
    "            history: Dict containing information about \n",
    "                     acc and loss.\n",
    "            file_name: The perfix name of current model - \n",
    "                       type: string. \n",
    "        '''\n",
    "        # Plot training & validation accuracy values\n",
    "        file_name_acc = file_name + \"_acc.png\"\n",
    "        plt.plot(history.history['acc'])\n",
    "        plt.plot(history.history['val_acc'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Val'], loc='upper left')\n",
    "        plt.savefig(file_name_acc)\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot training & validation loss values\n",
    "        file_name_loss = file_name + \"_loss.png\"\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Val'], loc='upper left')\n",
    "        plt.savefig(file_name_loss)\n",
    "        plt.close()\n",
    "        \n",
    "    def save_info(self, history, lr, beta, batch_size, file_name):\n",
    "        '''\n",
    "        This fuction for saving information of the model. The info are\n",
    "        accuracies, learning rate, beta, batch size.\n",
    "        params:\n",
    "            history: Dict containing all information  about current model.\n",
    "            lr     : Learning rate - type: float.\n",
    "            beta:  hyperparameters used in Adam, Adagrad, ...... -type: float.\n",
    "            batch_size: Number of input in each epoch type- float.\n",
    "            file_name: Perfix of the file name - type: string.\n",
    "        '''\n",
    "    \n",
    "        info = \" lr = {} \\n beta = {} \\n batch_size = {} \\n acc = {} \\n val_acc = {} \\n loss = {} \\n val_loss = {}\".format(\n",
    "        lr, beta, batch_size, history.history['acc'][-1], history.history['val_acc'][-1],\\\n",
    "            history.history['loss'][-1], history.history['val_loss'][-1])\n",
    "\n",
    "        file_name += \".txt\"\n",
    "        file = open(file_name, 'w')\n",
    "        file.write(info)\n",
    "        file.close()\n",
    "    \n",
    "    def train_model(self):\n",
    "        # Get data\n",
    "        raw_data = self.normalize_get_data()\n",
    "        train_x, train_y, val_x, val_y = raw_data[\"data\"]\n",
    "        \n",
    "        # Loop over optimizers\n",
    "        for batch_size in [64, 128, 256]:\n",
    "            for lr in [.01, .001, .0001]:\n",
    "                for beta in [.9, .99, .999]:\n",
    "                    print(\"batch_size is {}, lr is {}, beta is {}\".format(batch_size, lr, beta))\n",
    "                    cifarModel = self.model(train_x[0].shape)\n",
    "                    optimizer = keras.optimizers.Adam(lr=lr, beta_1=beta, beta_2=beta, epsilon=10e-8, decay=0.0, amsgrad=False)\n",
    "                    cifarModel.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "                    file_name = \"results/lr_\"+ str(lr) + \"_beta_\" + str(beta) + \"_batchsize_\" + str(batch_size)\n",
    "                    checkpointer = keras.callbacks.ModelCheckpoint(filepath=file_name +\".hdf5\", verbose=0, save_best_only=True)\n",
    "                    history = cifarModel.fit(x=train_x, y=keras.utils.to_categorical(train_y), \\\n",
    "                                   batch_size=batch_size, epochs=50, validation_data=(val_x, keras.utils.to_categorical(val_y)),\\\n",
    "                                   callbacks=[checkpointer])\n",
    "                    self.plot(history, file_name)\n",
    "                    self.save_info(history, lr, beta, batch_size, file_name)\n",
    "                    \n",
    "                  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size is 64, lr is 0.01, beta is 0.9\n",
      "Train on 5000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      " 256/5000 [>.............................] - ETA: 18:23 - loss: 2.7748 - acc: 0.1016"
     ]
    }
   ],
   "source": [
    "obj = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
